{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "870b3481-e591-4faa-bcf3-0e35b24d219a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Insurance Analytics - Data Foundation & Validation (Student Version)\n",
    "\n",
    "**Objective**: Load, validate, and prepare insurance datasets for downstream analytics pipeline\n",
    "\n",
    "**Business Goals:**\n",
    "- Load all insurance datasets with comprehensive validation\n",
    "- Create persistent database tables for ADF pipeline consumption\n",
    "- Ensure data quality for downstream analytics\n",
    "- Prepare optimized tables for Power BI integration\n",
    "\n",
    "**Professional Skills Demonstrated:**\n",
    "- Data quality assessment and validation\n",
    "- Business rules implementation\n",
    "- Database table management\n",
    "- Pipeline foundation establishment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c4e6c4-58ae-4cc0-9b7a-3b0753c84917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb4d752-3bf9-4056-a982-0cd896a54977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark session initialized\n\uD83D\uDCC2 Data Path: /mnt/coursedata/\n\uD83D\uDDC4️  Database: insurance_analytics\n✅ Database created: insurance_analytics\n"
     ]
    }
   ],
   "source": [
    "# BUSINESS CONTEXT: Proper setup ensures reliable data processing and analytics\n",
    "# Missing imports or incorrect configuration can cause entire pipeline failures\n",
    "\n",
    "# TODO: Import required libraries and configure Spark session\n",
    "# \n",
    "# Step 1: Import essential libraries\n",
    "# TODO: Import pandas as pd and numpy as np\n",
    "# TODO: Import SparkSession from pyspark.sql\n",
    "# TODO: Import all functions from pyspark.sql.functions (use *)\n",
    "# TODO: Import warnings and set warnings.filterwarnings('ignore')\n",
    "# \n",
    "# Step 2: Initialize Spark session\n",
    "# TODO: Create spark session with SparkSession.builder.appName(\"InsuranceDataFoundation\").getOrCreate()\n",
    "# TODO: Print \"✅ Spark session initialized\"\n",
    "# \n",
    "# Step 3: Configure database settings\n",
    "# TODO: Set DATA_PATH = \"/mnt/coursedata/\"\n",
    "# TODO: Set DATABASE_NAME = \"insurance_analytics\"\n",
    "# TODO: Print both path and database name\n",
    "# \n",
    "# EXPECTED OUTPUT: Spark session active with configuration variables set\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "# Import specific functions to avoid conflicts with Python built-ins\n",
    "from pyspark.sql.functions import col, sum as spark_sum, avg, count, when, lit, month, year, quarter, datediff, months_between, dayofweek\n",
    "from pyspark.sql.types import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"InsuranceDataFoundation\").getOrCreate()\n",
    "print(\"✅ Spark session initialized\")\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"/mnt/coursedata/\"\n",
    "DATABASE_NAME = \"insurance_analytics\"\n",
    "\n",
    "print(f\"\uD83D\uDCC2 Data Path: {DATA_PATH}\")\n",
    "print(f\"\uD83D\uDDC4️  Database: {DATABASE_NAME}\")\n",
    "\n",
    "# BUSINESS CONTEXT: Database creation ensures persistent storage for pipeline reliability\n",
    "# TODO: Create database for persistent tables\n",
    "# TODO: Use spark.sql() to CREATE DATABASE IF NOT EXISTS with DATABASE_NAME\n",
    "# TODO: Print success message with database name\n",
    "\n",
    "# Create database for persistent tables\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\")\n",
    "print(f\"✅ Database created: {DATABASE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2808966-e5c6-4032-a08a-f4041bdd428e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Load All Insurance Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdbf9a84-86af-466c-87a7-4f71117d132c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD04 LOADING INSURANCE DATASETS\n==================================================\n\uD83D\uDCCB Loading customer profiles...\n✅ Customers loaded: 15,000 records\n\uD83D\uDCCB Loading policy details...\n✅ Policies loaded: 75,000 records\n\uD83D\uDCCB Loading claims history...\n✅ Claims loaded: 10,643 records\n\uD83D\uDCCB Loading premium payments...\n✅ Payments loaded: 178,013 records\n\uD83D\uDCCB Loading customer interactions...\n✅ Interactions loaded: 30,000 records\n\uD83D\uDCCB Loading market rates...\n✅ Market rates loaded: 1 records\n\n\uD83C\uDFAF ALL DATASETS LOADED SUCCESSFULLY\n"
     ]
    }
   ],
   "source": [
    "# BUSINESS CONTEXT: Data loading is the foundation of all analytics\n",
    "# Poor data loading can invalidate entire business analysis\n",
    "\n",
    "print(\"\uD83D\uDD04 LOADING INSURANCE DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Load all insurance datasets with proper error handling\n",
    "# \n",
    "# Step 1: Load customer profiles\n",
    "# TODO: Load customers_df using spark.read with header=true and inferSchema=true\n",
    "# TODO: Use DATA_PATH + \"customer_profiles.csv\" as file path\n",
    "# TODO: Print loading message and count with f\"✅ Customers loaded: {customers_df.count():,} records\"\n",
    "# \n",
    "# Step 2: Load policy details\n",
    "# TODO: Load policies_df using same pattern as customers\n",
    "# TODO: Use \"policy_details.csv\" filename\n",
    "# TODO: Print loading message and count\n",
    "# \n",
    "# Step 3: Load claims history\n",
    "# TODO: Load claims_df using same pattern\n",
    "# TODO: Use \"claims_history.csv\" filename\n",
    "# TODO: Print loading message and count\n",
    "# \n",
    "# Step 4: Load premium payments\n",
    "# TODO: Load payments_df using same pattern\n",
    "# TODO: Use \"premium_payments.csv\" filename\n",
    "# TODO: Print loading message and count\n",
    "# \n",
    "# Step 5: Load customer interactions\n",
    "# TODO: Load interactions_df using same pattern\n",
    "# TODO: Use \"customer_interactions.csv\" filename\n",
    "# TODO: Print loading message and count\n",
    "# \n",
    "# EXPECTED OUTPUT: All 5 datasets loaded with record counts displayed\n",
    "\n",
    "# Load customers\n",
    "print(\"\uD83D\uDCCB Loading customer profiles...\")\n",
    "customers_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{DATA_PATH}customer_profiles.csv\")\n",
    "print(f\"✅ Customers loaded: {customers_df.count():,} records\")\n",
    "\n",
    "# Load policies\n",
    "print(\"\uD83D\uDCCB Loading policy details...\")\n",
    "policies_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{DATA_PATH}policy_details.csv\")\n",
    "print(f\"✅ Policies loaded: {policies_df.count():,} records\")\n",
    "\n",
    "# Load claims\n",
    "print(\"\uD83D\uDCCB Loading claims history...\")\n",
    "claims_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{DATA_PATH}claims_history.csv\")\n",
    "print(f\"✅ Claims loaded: {claims_df.count():,} records\")\n",
    "\n",
    "# Load payments\n",
    "print(\"\uD83D\uDCCB Loading premium payments...\")\n",
    "payments_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{DATA_PATH}premium_payments.csv\")\n",
    "print(f\"✅ Payments loaded: {payments_df.count():,} records\")\n",
    "\n",
    "# Load interactions\n",
    "print(\"\uD83D\uDCCB Loading customer interactions...\")\n",
    "interactions_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(f\"{DATA_PATH}customer_interactions.csv\")\n",
    "print(f\"✅ Interactions loaded: {interactions_df.count():,} records\")\n",
    "\n",
    "# BUSINESS CONTEXT: Market rates provide pricing benchmarks for competitive analysis\n",
    "# JSON files can be tricky to load and may require fallback data\n",
    "\n",
    "# TODO: Load market rates with error handling\n",
    "# \n",
    "# Step 1: Attempt to load JSON file\n",
    "# TODO: Use try/except block to load market_rates.json\n",
    "# TODO: In try block: load market_rates_df using spark.read.json()\n",
    "# TODO: Print success message with count\n",
    "# \n",
    "# Step 2: Create fallback data if JSON fails\n",
    "# TODO: In except block: create fallback market_rates_df using spark.createDataFrame()\n",
    "# TODO: Use this data structure: [(\"auto\", 0.02, 1.0, \"Standard auto insurance base rate\"), ...]\n",
    "# TODO: Include policy types: auto, home, life, health\n",
    "# TODO: Print fallback message\n",
    "# \n",
    "# EXPECTED OUTPUT: Market rates loaded (either from JSON or fallback)\n",
    "\n",
    "# Load market rates (JSON with fallback)\n",
    "print(\"\uD83D\uDCCB Loading market rates...\")\n",
    "try:\n",
    "    market_rates_df = spark.read.json(f\"{DATA_PATH}market_rates.json\")\n",
    "    print(f\"✅ Market rates loaded: {market_rates_df.count():,} records\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Market rates JSON issue: {e}\")\n",
    "    # Create fallback market rates\n",
    "    market_rates_df = spark.createDataFrame([\n",
    "        (\"auto\", 0.02, 1.0, \"Standard auto insurance base rate\"),\n",
    "        (\"home\", 0.008, 1.0, \"Standard home insurance base rate\"),\n",
    "        (\"life\", 0.005, 1.0, \"Standard life insurance base rate\"),\n",
    "        (\"health\", 0.08, 1.0, \"Standard health insurance base rate\")\n",
    "    ], [\"policy_type\", \"base_rate\", \"risk_multiplier\", \"description\"])\n",
    "    print(\"✅ Market rates: Using fallback data (4 records)\")\n",
    "\n",
    "print(f\"\\n\uD83C\uDFAF ALL DATASETS LOADED SUCCESSFULLY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9693ac1-7d4a-4fbd-acd6-6cf1aa2db93e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Comprehensive Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6e15bdf-8386-40f7-8801-13ad383f7731",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD0D COMPREHENSIVE DATA QUALITY VALIDATION\n==================================================\n\uD83C\uDFAF DATASET QUALITY VALIDATION\n\n\uD83D\uDCCA Customer Profiles - Quality Validation:\n   \uD83D\uDCCF Dimensions: 15,000 rows × 15 columns\n   ✅ All columns >95% complete\n   \uD83D\uDD11 customer_id: 15,000 unique (0.0% duplicates)\n\n\uD83D\uDCCA Policy Details - Quality Validation:\n   \uD83D\uDCCF Dimensions: 75,000 rows × 12 columns\n   ✅ All columns >95% complete\n   \uD83D\uDD11 policy_id: 75,000 unique (0.0% duplicates)\n\n\uD83D\uDCCA Claims History - Quality Validation:\n   \uD83D\uDCCF Dimensions: 10,643 rows × 10 columns\n   ⚠️  Quality Issues: 1\n      - days_to_settle: 88.7% complete\n   \uD83D\uDD11 claim_id: 10,643 unique (0.0% duplicates)\n\n\uD83D\uDCCA Premium Payments - Quality Validation:\n   \uD83D\uDCCF Dimensions: 178,013 rows × 8 columns\n   ✅ All columns >95% complete\n   \uD83D\uDD11 payment_id: 178,013 unique (0.0% duplicates)\n\n\uD83D\uDCCA Customer Interactions - Quality Validation:\n   \uD83D\uDCCF Dimensions: 30,000 rows × 9 columns\n   ✅ All columns >95% complete\n   \uD83D\uDD11 interaction_id: 30,000 unique (0.0% duplicates)\n\n\uD83D\uDCCA OVERALL DATA QUALITY SCORE: 99.0/100\n✅ DATA QUALITY: EXCELLENT\n"
     ]
    }
   ],
   "source": [
    "# BUSINESS CONTEXT: Data quality issues can invalidate entire business analysis\n",
    "# Poor data quality leads to incorrect business decisions and lost revenue\n",
    "\n",
    "print(\"\uD83D\uDD0D COMPREHENSIVE DATA QUALITY VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Create comprehensive data quality validation function\n",
    "# \n",
    "# Step 1: Define validation function\n",
    "# TODO: Create function validate_dataset_quality(df, dataset_name, key_columns=None)\n",
    "# TODO: Calculate total_rows using df.count() and total_cols using len(df.columns)\n",
    "# TODO: Print dimensions with f\"\uD83D\uDCCF Dimensions: {total_rows:,} rows × {total_cols} columns\"\n",
    "# \n",
    "# Step 2: Implement completeness checking\n",
    "# TODO: Create empty quality_issues list\n",
    "# TODO: Loop through df.columns to check each column\n",
    "# TODO: For each column, calculate null_count using df.filter(col(col_name).isNull()).count()\n",
    "# TODO: Calculate completeness percentage: ((total_rows - null_count) / total_rows) * 100\n",
    "# TODO: If completeness < 95%, add to quality_issues list\n",
    "# \n",
    "# Step 3: Validate key columns\n",
    "# TODO: If key_columns provided, loop through them\n",
    "# TODO: For each key column, count unique values with df.select(key_col).distinct().count()\n",
    "# TODO: Calculate duplicate_rate: ((total_rows - unique_count) / total_rows) * 100\n",
    "# TODO: Print key column statistics\n",
    "# \n",
    "# Step 4: Return quality score\n",
    "# TODO: Calculate quality_score = 100 - (len(quality_issues) * 5)\n",
    "# TODO: Return quality_score if > 0, otherwise return 0\n",
    "# \n",
    "# EXPECTED OUTPUT: Function that assesses data quality and returns score 0-100\n",
    "\n",
    "def validate_dataset_quality(df, dataset_name, key_columns=None):\n",
    "    \"\"\"Comprehensive data quality validation\"\"\"\n",
    "    print(f\"\\n\uD83D\uDCCA {dataset_name} - Quality Validation:\")\n",
    "    \n",
    "    # Basic metrics\n",
    "    total_rows = df.count()\n",
    "    total_cols = len(df.columns)\n",
    "    print(f\"   \uD83D\uDCCF Dimensions: {total_rows:,} rows × {total_cols} columns\")\n",
    "    \n",
    "    # Completeness check\n",
    "    quality_issues = []\n",
    "    for col_name in df.columns:\n",
    "        null_count = df.filter(col(col_name).isNull()).count()\n",
    "        completeness = ((total_rows - null_count) / total_rows) * 100\n",
    "        \n",
    "        if completeness < 95:\n",
    "            quality_issues.append(f\"{col_name}: {completeness:.1f}% complete\")\n",
    "    \n",
    "    if quality_issues:\n",
    "        print(f\"   ⚠️  Quality Issues: {len(quality_issues)}\")\n",
    "        for issue in quality_issues[:3]:  # Show first 3\n",
    "            print(f\"      - {issue}\")\n",
    "    else:\n",
    "        print(\"   ✅ All columns >95% complete\")\n",
    "    \n",
    "    # Key column validation\n",
    "    if key_columns:\n",
    "        for key_col in key_columns:\n",
    "            if key_col in df.columns:\n",
    "                unique_count = df.select(key_col).distinct().count()\n",
    "                duplicate_rate = ((total_rows - unique_count) / total_rows) * 100\n",
    "                print(f\"   \uD83D\uDD11 {key_col}: {unique_count:,} unique ({duplicate_rate:.1f}% duplicates)\")\n",
    "    \n",
    "    # Return quality score\n",
    "    quality_score = 100 - (len(quality_issues) * 5)  # Deduct 5 points per issue\n",
    "    return quality_score if quality_score > 0 else 0\n",
    "\n",
    "# BUSINESS CONTEXT: Each dataset has different quality requirements and key identifiers\n",
    "# Systematic validation ensures consistent data quality across all datasets\n",
    "\n",
    "# TODO: Validate all datasets using the validation function\n",
    "# \n",
    "# Step 1: Validate customer profiles\n",
    "# TODO: Call validate_dataset_quality() for customers_df with key_columns=[\"customer_id\"]\n",
    "# TODO: Store result in customers_score\n",
    "# \n",
    "# Step 2: Validate policy details\n",
    "# TODO: Call validate_dataset_quality() for policies_df with key_columns=[\"policy_id\"]\n",
    "# TODO: Store result in policies_score\n",
    "# \n",
    "# Step 3: Validate claims history\n",
    "# TODO: Call validate_dataset_quality() for claims_df with key_columns=[\"claim_id\"]\n",
    "# TODO: Store result in claims_score\n",
    "# \n",
    "# Step 4: Validate premium payments\n",
    "# TODO: Call validate_dataset_quality() for payments_df with key_columns=[\"payment_id\"]\n",
    "# TODO: Store result in payments_score\n",
    "# \n",
    "# Step 5: Validate customer interactions\n",
    "# TODO: Call validate_dataset_quality() for interactions_df with key_columns=[\"interaction_id\"]\n",
    "# TODO: Store result in interactions_score\n",
    "# \n",
    "# EXPECTED OUTPUT: Quality scores for all 5 datasets with detailed validation results\n",
    "\n",
    "# Validate each dataset\n",
    "print(\"\uD83C\uDFAF DATASET QUALITY VALIDATION\")\n",
    "customers_score = validate_dataset_quality(customers_df, \"Customer Profiles\", [\"customer_id\"])\n",
    "policies_score = validate_dataset_quality(policies_df, \"Policy Details\", [\"policy_id\"])\n",
    "claims_score = validate_dataset_quality(claims_df, \"Claims History\", [\"claim_id\"])\n",
    "payments_score = validate_dataset_quality(payments_df, \"Premium Payments\", [\"payment_id\"])\n",
    "interactions_score = validate_dataset_quality(interactions_df, \"Customer Interactions\", [\"interaction_id\"])\n",
    "\n",
    "# BUSINESS CONTEXT: Overall data quality score determines if pipeline can proceed safely\n",
    "# Scores below 70% indicate serious data quality issues requiring attention\n",
    "\n",
    "# TODO: Calculate overall quality assessment\n",
    "# \n",
    "# Step 1: Calculate average quality score\n",
    "# TODO: Calculate avg_quality by averaging all 5 quality scores\n",
    "# TODO: Print overall quality score with f\"\uD83D\uDCCA OVERALL DATA QUALITY SCORE: {avg_quality:.1f}/100\"\n",
    "# \n",
    "# Step 2: Assess quality level and set flag\n",
    "# TODO: If avg_quality >= 85, print \"✅ DATA QUALITY: EXCELLENT\" and set data_quality_ok = True\n",
    "# TODO: Elif avg_quality >= 70, print \"⚠️  DATA QUALITY: ACCEPTABLE\" and set data_quality_ok = True\n",
    "# TODO: Else print \"❌ DATA QUALITY: NEEDS ATTENTION\" and set data_quality_ok = False\n",
    "# \n",
    "# EXPECTED OUTPUT: Overall quality assessment with pass/fail determination\n",
    "\n",
    "# Overall quality assessment\n",
    "avg_quality = (customers_score + policies_score + claims_score + payments_score + interactions_score) / 5\n",
    "print(f\"\\n\uD83D\uDCCA OVERALL DATA QUALITY SCORE: {avg_quality:.1f}/100\")\n",
    "\n",
    "if avg_quality >= 85:\n",
    "    print(\"✅ DATA QUALITY: EXCELLENT\")\n",
    "    data_quality_ok = True\n",
    "elif avg_quality >= 70:\n",
    "    print(\"⚠️  DATA QUALITY: ACCEPTABLE\")\n",
    "    data_quality_ok = True\n",
    "else:\n",
    "    print(\"❌ DATA QUALITY: NEEDS ATTENTION\")\n",
    "    data_quality_ok = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6869a46-0b40-409a-acd0-24a9f3cf2c9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Business Rules and Relationship Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f61b4ba-11e8-4a57-951f-a696c618f35f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCB BUSINESS RULES & RELATIONSHIP VALIDATION\n==================================================\n\uD83D\uDD17 Test 1: Customer-Policy Relationship\n   Total policies: 75,000\n   Valid customer links: 75,000\n   Link integrity: 100.00%\n   ✅ PASS: Customer-Policy relationships are solid\n\n\uD83D\uDD17 Test 2: Policy-Claims Relationship\n   Total claims: 10,643\n   Valid policy links: 10,643\n   Link integrity: 100.00%\n   ✅ PASS: Policy-Claims relationships are solid\n\n\uD83D\uDCB0 Test 3: Claims vs Coverage Validation\n   Claims with coverage data: 10,643\n   Claims exceeding coverage: 0\n   ✅ PASS: All claims within coverage limits\n\n\uD83D\uDCB5 Test 4: Premium Amount Validation\n   Invalid premiums (≤0 or >$100K): 141\n   ❌ FAIL: 141 policies have invalid premiums\n\n\uD83D\uDCC5 Test 5: Date Logic Validation\n   Invalid policy date sequences: 0\n   Claims with unrealistic dates: 0\n   ✅ PASS: All date logic is valid\n\n\uD83D\uDCCA BUSINESS RULES SUMMARY:\n   Rules passed: 4/5\n   Pass rate: 80.0%\n   ✅ BUSINESS RULES: PASSED\n"
     ]
    }
   ],
   "source": [
    "# BUSINESS CONTEXT: Business rules ensure data integrity and logical consistency\n",
    "# Broken relationships between tables can lead to incorrect analytics and business decisions\n",
    "\n",
    "print(\"\uD83D\uDCCB BUSINESS RULES & RELATIONSHIP VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Test customer-policy relationship integrity\n",
    "# \n",
    "# Step 1: Calculate relationship integrity\n",
    "# TODO: Get total_policies count from policies_df\n",
    "# TODO: Join policies_df with customers_df on \"customer_id\" using inner join\n",
    "# TODO: Count the joined result as valid_customer_policies\n",
    "# TODO: Calculate customer_link_integrity = (valid_customer_policies / total_policies) * 100\n",
    "# \n",
    "# Step 2: Print relationship statistics\n",
    "# TODO: Print total policies, valid customer links, and link integrity percentage\n",
    "# \n",
    "# Step 3: Assess relationship quality\n",
    "# TODO: If customer_link_integrity >= 99, print \"✅ PASS\" and set customer_policy_ok = True\n",
    "# TODO: Else print \"❌ FAIL\" and set customer_policy_ok = False\n",
    "# \n",
    "# EXPECTED OUTPUT: Customer-policy relationship integrity assessment\n",
    "\n",
    "# Test 1: Customer-Policy Relationship Integrity\n",
    "print(\"\uD83D\uDD17 Test 1: Customer-Policy Relationship\")\n",
    "total_policies = policies_df.count()\n",
    "valid_customer_policies = policies_df.join(customers_df, \"customer_id\", \"inner\").count()\n",
    "customer_link_integrity = (valid_customer_policies / total_policies) * 100\n",
    "\n",
    "print(f\"   Total policies: {total_policies:,}\")\n",
    "print(f\"   Valid customer links: {valid_customer_policies:,}\")\n",
    "print(f\"   Link integrity: {customer_link_integrity:.2f}%\")\n",
    "\n",
    "if customer_link_integrity >= 99:\n",
    "    print(\"   ✅ PASS: Customer-Policy relationships are solid\")\n",
    "    customer_policy_ok = True\n",
    "else:\n",
    "    print(\"   ❌ FAIL: Customer-Policy relationship issues detected\")\n",
    "    customer_policy_ok = False\n",
    "\n",
    "# BUSINESS CONTEXT: Policy-claims relationships are critical for accurate loss analysis\n",
    "# Orphaned claims (not linked to policies) skew loss ratios and financial metrics\n",
    "\n",
    "# TODO: Test policy-claims relationship integrity\n",
    "# \n",
    "# Step 1: Calculate relationship integrity\n",
    "# TODO: Get total_claims count from claims_df\n",
    "# TODO: Join claims_df with policies_df on \"policy_id\" using inner join\n",
    "# TODO: Count the joined result as valid_policy_claims\n",
    "# TODO: Calculate policy_link_integrity = (valid_policy_claims / total_claims) * 100\n",
    "# \n",
    "# Step 2: Print relationship statistics\n",
    "# TODO: Print total claims, valid policy links, and link integrity percentage\n",
    "# \n",
    "# Step 3: Assess relationship quality\n",
    "# TODO: If policy_link_integrity >= 99, print \"✅ PASS\" and set policy_claims_ok = True\n",
    "# TODO: Else print \"❌ FAIL\" and set policy_claims_ok = False\n",
    "# \n",
    "# EXPECTED OUTPUT: Policy-claims relationship integrity assessment\n",
    "\n",
    "# Test 2: Policy-Claims Relationship Integrity\n",
    "print(\"\\n\uD83D\uDD17 Test 2: Policy-Claims Relationship\")\n",
    "total_claims = claims_df.count()\n",
    "valid_policy_claims = claims_df.join(policies_df, \"policy_id\", \"inner\").count()\n",
    "policy_link_integrity = (valid_policy_claims / total_claims) * 100\n",
    "\n",
    "print(f\"   Total claims: {total_claims:,}\")\n",
    "print(f\"   Valid policy links: {valid_policy_claims:,}\")\n",
    "print(f\"   Link integrity: {policy_link_integrity:.2f}%\")\n",
    "\n",
    "if policy_link_integrity >= 99:\n",
    "    print(\"   ✅ PASS: Policy-Claims relationships are solid\")\n",
    "    policy_claims_ok = True\n",
    "else:\n",
    "    print(\"   ❌ FAIL: Policy-Claims relationship issues detected\")\n",
    "    policy_claims_ok = False\n",
    "\n",
    "# BUSINESS CONTEXT: Claims should never exceed coverage amounts\n",
    "# This business rule protects against fraudulent or erroneous claims\n",
    "\n",
    "# TODO: Test claims vs coverage amounts business rule\n",
    "# \n",
    "# Step 1: Join claims with coverage data\n",
    "# TODO: Join claims_df with policies_df selecting only \"policy_id\" and \"coverage_amount\"\n",
    "# TODO: Use inner join on \"policy_id\" and store as claims_coverage\n",
    "# \n",
    "# Step 2: Find excessive claims\n",
    "# TODO: Filter claims_coverage where claim_amount > coverage_amount\n",
    "# TODO: Count the filtered result as excessive_claims\n",
    "# \n",
    "# Step 3: Print validation results\n",
    "# TODO: Print claims with coverage data count and excessive claims count\n",
    "# \n",
    "# Step 4: Assess business rule compliance\n",
    "# TODO: If excessive_claims == 0, print \"✅ PASS\" and set claims_coverage_ok = True\n",
    "# TODO: Else print \"❌ FAIL\" with excessive claims count and set claims_coverage_ok = False\n",
    "# \n",
    "# EXPECTED OUTPUT: Claims vs coverage validation with business rule compliance\n",
    "\n",
    "# Test 3: Claims vs Coverage Amounts\n",
    "print(\"\\n\uD83D\uDCB0 Test 3: Claims vs Coverage Validation\")\n",
    "claims_coverage = claims_df.join(\n",
    "    policies_df.select(\"policy_id\", \"coverage_amount\"), \n",
    "    \"policy_id\", \"inner\"\n",
    ")\n",
    "excessive_claims = claims_coverage.filter(col(\"claim_amount\") > col(\"coverage_amount\")).count()\n",
    "\n",
    "print(f\"   Claims with coverage data: {claims_coverage.count():,}\")\n",
    "print(f\"   Claims exceeding coverage: {excessive_claims:,}\")\n",
    "\n",
    "if excessive_claims == 0:\n",
    "    print(\"   ✅ PASS: All claims within coverage limits\")\n",
    "    claims_coverage_ok = True\n",
    "else:\n",
    "    print(f\"   ❌ FAIL: {excessive_claims} claims exceed coverage\")\n",
    "    claims_coverage_ok = False\n",
    "\n",
    "# BUSINESS CONTEXT: Premium amounts must be reasonable to ensure data integrity\n",
    "# Unrealistic premiums indicate data quality issues or system errors\n",
    "\n",
    "# TODO: Test premium amount reasonableness\n",
    "# \n",
    "# Step 1: Find invalid premiums\n",
    "# TODO: Filter policies_df for premium_amount <= 0 OR premium_amount > 100000\n",
    "# TODO: Count the filtered result as invalid_premiums\n",
    "# \n",
    "# Step 2: Print validation results\n",
    "# TODO: Print count of invalid premiums\n",
    "# \n",
    "# Step 3: Assess business rule compliance\n",
    "# TODO: If invalid_premiums == 0, print \"✅ PASS\" and set premium_amounts_ok = True\n",
    "# TODO: Else print \"❌ FAIL\" with invalid premiums count and set premium_amounts_ok = False\n",
    "# \n",
    "# EXPECTED OUTPUT: Premium amount validation with business rule compliance\n",
    "\n",
    "# Test 4: Premium Amount Validation\n",
    "print(\"\\n\uD83D\uDCB5 Test 4: Premium Amount Validation\")\n",
    "invalid_premiums = policies_df.filter(\n",
    "    (col(\"premium_amount\") <= 0) | \n",
    "    (col(\"premium_amount\") > 100000)\n",
    ").count()\n",
    "\n",
    "print(f\"   Invalid premiums (≤0 or >$100K): {invalid_premiums:,}\")\n",
    "\n",
    "if invalid_premiums == 0:\n",
    "    print(\"   ✅ PASS: All premium amounts are reasonable\")\n",
    "    premium_amounts_ok = True\n",
    "else:\n",
    "    print(f\"   ❌ FAIL: {invalid_premiums} policies have invalid premiums\")\n",
    "    premium_amounts_ok = False\n",
    "\n",
    "# BUSINESS CONTEXT: Date logic ensures temporal consistency in business data\n",
    "# Invalid dates can cause timeline analysis and aging calculations to fail\n",
    "\n",
    "# TODO: Test date logic validation\n",
    "# \n",
    "# Step 1: Check policy date sequences\n",
    "# TODO: Filter policies_df where start_date >= end_date\n",
    "# TODO: Count the filtered result as invalid_policy_dates\n",
    "# \n",
    "# Step 2: Check claim date reasonableness\n",
    "# TODO: Filter claims_df where claim_date < \"2020-01-01\"\n",
    "# TODO: Count the filtered result as invalid_claim_dates\n",
    "# \n",
    "# Step 3: Print validation results\n",
    "# TODO: Print invalid policy date sequences and invalid claim dates\n",
    "# \n",
    "# Step 4: Assess date logic compliance\n",
    "# TODO: If both counts == 0, print \"✅ PASS\" and set date_logic_ok = True\n",
    "# TODO: Else print \"❌ FAIL\" and set date_logic_ok = False\n",
    "# \n",
    "# EXPECTED OUTPUT: Date logic validation with business rule compliance\n",
    "\n",
    "# Test 5: Date Logic Validation\n",
    "print(\"\\n\uD83D\uDCC5 Test 5: Date Logic Validation\")\n",
    "invalid_policy_dates = policies_df.filter(col(\"start_date\") >= col(\"end_date\")).count()\n",
    "invalid_claim_dates = claims_df.filter(col(\"claim_date\") < lit(\"2020-01-01\")).count()\n",
    "\n",
    "print(f\"   Invalid policy date sequences: {invalid_policy_dates:,}\")\n",
    "print(f\"   Claims with unrealistic dates: {invalid_claim_dates:,}\")\n",
    "\n",
    "if invalid_policy_dates == 0 and invalid_claim_dates == 0:\n",
    "    print(\"   ✅ PASS: All date logic is valid\")\n",
    "    date_logic_ok = True\n",
    "else:\n",
    "    print(\"   ❌ FAIL: Date logic issues detected\")\n",
    "    date_logic_ok = False\n",
    "\n",
    "# BUSINESS CONTEXT: Business rules pass rate determines pipeline reliability\n",
    "# Failed business rules indicate data issues that must be addressed\n",
    "\n",
    "# TODO: Summarize business rules validation\n",
    "# \n",
    "# Step 1: Create business rules list\n",
    "# TODO: Create list business_rules containing all 5 boolean results\n",
    "# TODO: Calculate passed_rules using sum() function\n",
    "# TODO: Set total_rules = len(business_rules)\n",
    "# \n",
    "# Step 2: Print business rules summary\n",
    "# TODO: Print rules passed, total rules, and pass rate percentage\n",
    "# \n",
    "# Step 3: Assess overall business rules compliance\n",
    "# TODO: If passed_rules >= 4, print \"✅ BUSINESS RULES: PASSED\" and set business_rules_ok = True\n",
    "# TODO: Else print \"❌ BUSINESS RULES: FAILED\" and set business_rules_ok = False\n",
    "# \n",
    "# EXPECTED OUTPUT: Business rules summary with overall pass/fail determination\n",
    "\n",
    "# Business Rules Summary\n",
    "business_rules = [\n",
    "    customer_policy_ok, policy_claims_ok, claims_coverage_ok, \n",
    "    premium_amounts_ok, date_logic_ok\n",
    "]\n",
    "# Now we can use Python's built-in sum() safely\n",
    "passed_rules = sum(business_rules)\n",
    "total_rules = len(business_rules)\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA BUSINESS RULES SUMMARY:\")\n",
    "print(f\"   Rules passed: {passed_rules}/{total_rules}\")\n",
    "print(f\"   Pass rate: {(passed_rules/total_rules)*100:.1f}%\")\n",
    "\n",
    "if passed_rules >= 4:\n",
    "    print(\"   ✅ BUSINESS RULES: PASSED\")\n",
    "    business_rules_ok = True\n",
    "else:\n",
    "    print(\"   ❌ BUSINESS RULES: FAILED\")\n",
    "    business_rules_ok = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8991f9d7-c982-4e07-b15d-aefb4d1394cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Create Persistent Database Tables for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3362e2f-ddfb-46b6-8abd-36ba0d71ffc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83C\uDFD7️  CREATING PERSISTENT DATABASE TABLES\n==================================================\n\uD83D\uDCCB Creating foundation tables for pipeline...\n✅ Created & Optimized: customers (15,000 records) - Customer demographics and profiles\n✅ Created & Optimized: policies (75,000 records) - Insurance policy details and coverage\n✅ Created & Optimized: claims (10,643 records) - Insurance claims history and settlements\n✅ Created & Optimized: payments (178,013 records) - Premium payment transactions\n✅ Created & Optimized: interactions (30,000 records) - Customer service interactions and satisfaction\n❌ Failed to create market_rates: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().\n\n\uD83D\uDCCA TABLE CREATION SUMMARY:\n   Successfully created: 5/6 tables\n   ✅ CORE TABLES: Successfully created for pipeline\n"
     ]
    }
   ],
   "source": [
    "# BUSINESS CONTEXT: Persistent tables enable reliable ADF pipeline execution\n",
    "# Temporary views disappear between notebook runs, breaking pipeline dependencies\n",
    "\n",
    "print(\"\uD83C\uDFD7️  CREATING PERSISTENT DATABASE TABLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Create function for persistent table creation\n",
    "# \n",
    "# Step 1: Define table creation function\n",
    "# TODO: Create function create_persistent_table(df, table_name, description, optimize_columns=None)\n",
    "# TODO: Use try/except block for error handling\n",
    "# \n",
    "# Step 2: Write table in try block\n",
    "# TODO: Use df.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE_NAME}.{table_name}\")\n",
    "# TODO: Get record_count using df.count()\n",
    "# \n",
    "# Step 3: Handle optimization\n",
    "# TODO: If optimize_columns provided, try to run spark.sql(f\"OPTIMIZE {DATABASE_NAME}.{table_name}\")\n",
    "# TODO: Print success message with table name, record count, and description\n",
    "# \n",
    "# Step 4: Handle errors\n",
    "# TODO: In except block, print error message and return False\n",
    "# TODO: In success case, return True\n",
    "# \n",
    "# EXPECTED OUTPUT: Function that creates persistent tables with error handling\n",
    "\n",
    "def create_persistent_table(df, table_name, description, optimize_columns=None):\n",
    "    \"\"\"Create persistent table with optimization\"\"\"\n",
    "    try:\n",
    "        # Write table\n",
    "        df.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE_NAME}.{table_name}\")\n",
    "        \n",
    "        # Get record count\n",
    "        record_count = df.count()\n",
    "        \n",
    "        # Basic optimization if specified\n",
    "        if optimize_columns:\n",
    "            try:\n",
    "                spark.sql(f\"OPTIMIZE {DATABASE_NAME}.{table_name}\")\n",
    "                print(f\"✅ Created & Optimized: {table_name} ({record_count:,} records) - {description}\")\n",
    "            except:\n",
    "                print(f\"✅ Created: {table_name} ({record_count:,} records) - {description}\")\n",
    "        else:\n",
    "            print(f\"✅ Created: {table_name} ({record_count:,} records) - {description}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to create {table_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "print(\"\uD83D\uDCCB Creating foundation tables for pipeline...\")\n",
    "\n",
    "# BUSINESS CONTEXT: Core business entity tables provide foundation for all analytics\n",
    "# These tables must be created successfully for downstream notebooks to function\n",
    "\n",
    "# TODO: Create all persistent tables for pipeline\n",
    "# \n",
    "# Step 1: Create customers table\n",
    "# TODO: Call create_persistent_table() for customers_df\n",
    "# TODO: Use table_name=\"customers\", description=\"Customer demographics and profiles\"\n",
    "# TODO: Use optimize_columns=[\"customer_id\"]\n",
    "# TODO: Store result in tables_created list\n",
    "# \n",
    "# Step 2: Create policies table\n",
    "# TODO: Call create_persistent_table() for policies_df\n",
    "# TODO: Use table_name=\"policies\", description=\"Insurance policy details and coverage\"\n",
    "# TODO: Use optimize_columns=[\"policy_id\", \"customer_id\"]\n",
    "# TODO: Append result to tables_created list\n",
    "# \n",
    "# Step 3: Create claims table\n",
    "# TODO: Call create_persistent_table() for claims_df\n",
    "# TODO: Use table_name=\"claims\", description=\"Insurance claims history and settlements\"\n",
    "# TODO: Use optimize_columns=[\"claim_id\", \"policy_id\"]\n",
    "# TODO: Append result to tables_created list\n",
    "# \n",
    "# Step 4: Create payments table\n",
    "# TODO: Call create_persistent_table() for payments_df\n",
    "# TODO: Use table_name=\"payments\", description=\"Premium payment transactions\"\n",
    "# TODO: Use optimize_columns=[\"payment_id\", \"customer_id\"]\n",
    "# TODO: Append result to tables_created list\n",
    "# \n",
    "# Step 5: Create interactions table\n",
    "# TODO: Call create_persistent_table() for interactions_df\n",
    "# TODO: Use table_name=\"interactions\", description=\"Customer service interactions and satisfaction\"\n",
    "# TODO: Use optimize_columns=[\"interaction_id\", \"customer_id\"]\n",
    "# TODO: Append result to tables_created list\n",
    "# \n",
    "# Step 6: Create market_rates table\n",
    "# TODO: Call create_persistent_table() for market_rates_df\n",
    "# TODO: Use table_name=\"market_rates\", description=\"Insurance market rates and benchmarks\"\n",
    "# TODO: No optimize_columns needed for this table\n",
    "# TODO: Append result to tables_created list\n",
    "# \n",
    "# EXPECTED OUTPUT: 6 persistent tables created successfully\n",
    "\n",
    "# Create all persistent tables\n",
    "tables_created = []\n",
    "\n",
    "# Core business entity tables\n",
    "tables_created.append(create_persistent_table(\n",
    "    customers_df, \n",
    "    \"customers\", \n",
    "    \"Customer demographics and profiles\",\n",
    "    [\"customer_id\"]\n",
    "))\n",
    "\n",
    "tables_created.append(create_persistent_table(\n",
    "    policies_df, \n",
    "    \"policies\", \n",
    "    \"Insurance policy details and coverage\",\n",
    "    [\"policy_id\", \"customer_id\"]\n",
    "))\n",
    "\n",
    "tables_created.append(create_persistent_table(\n",
    "    claims_df, \n",
    "    \"claims\", \n",
    "    \"Insurance claims history and settlements\",\n",
    "    [\"claim_id\", \"policy_id\"]\n",
    "))\n",
    "\n",
    "tables_created.append(create_persistent_table(\n",
    "    payments_df, \n",
    "    \"payments\", \n",
    "    \"Premium payment transactions\",\n",
    "    [\"payment_id\", \"customer_id\"]\n",
    "))\n",
    "\n",
    "tables_created.append(create_persistent_table(\n",
    "    interactions_df, \n",
    "    \"interactions\", \n",
    "    \"Customer service interactions and satisfaction\",\n",
    "    [\"interaction_id\", \"customer_id\"]\n",
    "))\n",
    "\n",
    "tables_created.append(create_persistent_table(\n",
    "    market_rates_df, \n",
    "    \"market_rates\", \n",
    "    \"Insurance market rates and benchmarks\"\n",
    "))\n",
    "\n",
    "# BUSINESS CONTEXT: Table creation success determines pipeline readiness\n",
    "# Failed table creation prevents downstream notebooks from executing\n",
    "\n",
    "# TODO: Assess table creation success\n",
    "# \n",
    "# Step 1: Calculate successful table creation count\n",
    "# TODO: Sum the tables_created list to get successful_tables count\n",
    "# TODO: Print table creation summary with successful_tables and total attempted\n",
    "# \n",
    "# Step 2: Assess table creation readiness\n",
    "# TODO: If successful_tables >= 5, print \"✅ CORE TABLES: Successfully created\" and set table_creation_ok = True\n",
    "# TODO: Else print \"❌ CORE TABLES: Critical failures detected\" and set table_creation_ok = False\n",
    "# \n",
    "# EXPECTED OUTPUT: Table creation assessment with pipeline readiness determination\n",
    "\n",
    "# Summary\n",
    "successful_tables = sum(tables_created)\n",
    "print(f\"\\n\uD83D\uDCCA TABLE CREATION SUMMARY:\")\n",
    "print(f\"   Successfully created: {successful_tables}/{len(tables_created)} tables\")\n",
    "\n",
    "if successful_tables >= 5:  # Core tables are essential\n",
    "    print(\"   ✅ CORE TABLES: Successfully created for pipeline\")\n",
    "    table_creation_ok = True\n",
    "else:\n",
    "    print(\"   ❌ CORE TABLES: Critical failures detected\")\n",
    "    table_creation_ok = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f45899a-a5a2-41d9-be62-fbe210fad3de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Create Optimized Views for Power BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73e765ee-2b75-4bc7-ae64-06d49ef06a6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCCA CREATING OPTIMIZED VIEWS FOR POWER BI\n==================================================\n\uD83D\uDCCB Creating customer summary view...\n✅ Customer summary view created\n\uD83D\uDCCB Creating policy performance view...\n✅ Policy performance view created\n\uD83D\uDCCB Creating monthly trends view...\n✅ Monthly trends view created\n\n\uD83D\uDCCA POWER BI VIEWS SUMMARY:\n   ✅ customer_summary - Customer 360 view\n   ✅ policy_performance - Policy type analytics\n   ✅ monthly_trends - Time series analysis\n"
     ]
    }
   ],
   "source": [
    "# BUSINESS CONTEXT: Power BI requires optimized views for fast dashboard performance\n",
    "# Pre-aggregated views reduce query time and improve user experience\n",
    "\n",
    "print(\"\uD83D\uDCCA CREATING OPTIMIZED VIEWS FOR POWER BI\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Create customer summary view for Power BI\n",
    "# \n",
    "# Step 1: Create customer summary SQL query\n",
    "# TODO: Create customer_summary_sql variable with CREATE OR REPLACE VIEW statement\n",
    "# TODO: SELECT customer data: customer_id, first_name, last_name, email, state, income, credit_score, risk_category, acquisition_date\n",
    "# TODO: Add aggregated policy data: COUNT(DISTINCT p.policy_id) as total_policies, SUM(p.premium_amount) as total_premium\n",
    "# TODO: Add aggregated claims data: COUNT(DISTINCT cl.claim_id) as total_claims, COALESCE(SUM(cl.claim_amount), 0) as total_claim_amount\n",
    "# TODO: Add interaction data: COALESCE(AVG(i.satisfaction_score), 3.0) as avg_satisfaction\n",
    "# TODO: FROM customers c with LEFT JOINs to policies p, claims cl, interactions i\n",
    "# TODO: GROUP BY all customer fields\n",
    "# \n",
    "# Step 2: Execute the SQL query\n",
    "# TODO: Use spark.sql(customer_summary_sql) to create the view\n",
    "# TODO: Print success message \"✅ Customer summary view created\"\n",
    "# \n",
    "# EXPECTED OUTPUT: Customer summary view created for Power BI integration\n",
    "\n",
    "# Customer Summary View (for Power BI connection)\n",
    "print(\"\uD83D\uDCCB Creating customer summary view...\")\n",
    "customer_summary_sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW {DATABASE_NAME}.customer_summary AS\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.first_name,\n",
    "    c.last_name,\n",
    "    c.email,\n",
    "    c.state,\n",
    "    c.income,\n",
    "    c.credit_score,\n",
    "    c.risk_category,\n",
    "    c.acquisition_date,\n",
    "    COUNT(DISTINCT p.policy_id) as total_policies,\n",
    "    SUM(p.premium_amount) as total_premium,\n",
    "    COUNT(DISTINCT cl.claim_id) as total_claims,\n",
    "    COALESCE(SUM(cl.claim_amount), 0) as total_claim_amount,\n",
    "    COALESCE(AVG(i.satisfaction_score), 3.0) as avg_satisfaction\n",
    "FROM {DATABASE_NAME}.customers c\n",
    "LEFT JOIN {DATABASE_NAME}.policies p ON c.customer_id = p.customer_id\n",
    "LEFT JOIN {DATABASE_NAME}.claims cl ON p.policy_id = cl.policy_id\n",
    "LEFT JOIN {DATABASE_NAME}.interactions i ON c.customer_id = i.customer_id\n",
    "GROUP BY c.customer_id, c.first_name, c.last_name, c.email, c.state, \n",
    "         c.income, c.credit_score, c.risk_category, c.acquisition_date\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(customer_summary_sql)\n",
    "print(\"✅ Customer summary view created\")\n",
    "\n",
    "# BUSINESS CONTEXT: Policy performance analysis enables strategic business decisions\n",
    "# Aggregated policy metrics support executive dashboard visualizations\n",
    "\n",
    "# TODO: Create policy performance view for Power BI\n",
    "# \n",
    "# Step 1: Create policy performance SQL query\n",
    "# TODO: Create policy_performance_sql variable with CREATE OR REPLACE VIEW statement\n",
    "# TODO: SELECT policy data: policy_type, policy_status\n",
    "# TODO: Add aggregated metrics: COUNT(*) as policy_count, SUM(premium_amount) as total_premium, AVG(premium_amount) as avg_premium\n",
    "# TODO: Add coverage data: SUM(coverage_amount) as total_coverage\n",
    "# TODO: Add claims data: COUNT(DISTINCT cl.claim_id) as total_claims, COALESCE(SUM(cl.claim_amount), 0) as total_claim_amount\n",
    "# TODO: Calculate loss ratio: CASE WHEN SUM(premium_amount) > 0 THEN (claims/premiums)*100 ELSE 0 END as loss_ratio\n",
    "# TODO: FROM policies p with LEFT JOIN to claims cl\n",
    "# TODO: GROUP BY policy_type, policy_status\n",
    "# \n",
    "# Step 2: Execute the SQL query\n",
    "# TODO: Use spark.sql(policy_performance_sql) to create the view\n",
    "# TODO: Print success message \"✅ Policy performance view created\"\n",
    "# \n",
    "# EXPECTED OUTPUT: Policy performance view created for Power BI dashboard\n",
    "\n",
    "# Policy Performance View (for Power BI dashboard)\n",
    "print(\"\uD83D\uDCCB Creating policy performance view...\")\n",
    "policy_performance_sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW {DATABASE_NAME}.policy_performance AS\n",
    "SELECT \n",
    "    p.policy_type,\n",
    "    p.policy_status,\n",
    "    COUNT(*) as policy_count,\n",
    "    SUM(p.premium_amount) as total_premium,\n",
    "    AVG(p.premium_amount) as avg_premium,\n",
    "    SUM(p.coverage_amount) as total_coverage,\n",
    "    COUNT(DISTINCT cl.claim_id) as total_claims,\n",
    "    COALESCE(SUM(cl.claim_amount), 0) as total_claim_amount,\n",
    "    CASE \n",
    "        WHEN SUM(p.premium_amount) > 0 \n",
    "        THEN (COALESCE(SUM(cl.claim_amount), 0) / SUM(p.premium_amount)) * 100\n",
    "        ELSE 0 \n",
    "    END as loss_ratio\n",
    "FROM {DATABASE_NAME}.policies p\n",
    "LEFT JOIN {DATABASE_NAME}.claims cl ON p.policy_id = cl.policy_id\n",
    "GROUP BY p.policy_type, p.policy_status\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(policy_performance_sql)\n",
    "print(\"✅ Policy performance view created\")\n",
    "\n",
    "# BUSINESS CONTEXT: Time series analysis reveals seasonal patterns and trends\n",
    "# Monthly trends support forecasting and resource planning\n",
    "\n",
    "# TODO: Create monthly trends view for Power BI\n",
    "# \n",
    "# Step 1: Create monthly trends SQL query\n",
    "# TODO: Create monthly_trends_sql variable with CREATE OR REPLACE VIEW statement\n",
    "# TODO: SELECT time fields: YEAR(cl.claim_date) as claim_year, MONTH(cl.claim_date) as claim_month\n",
    "# TODO: Add aggregated metrics: COUNT(*) as claims_count, SUM(cl.claim_amount) as total_claim_amount\n",
    "# TODO: Add calculated fields: AVG(cl.claim_amount) as avg_claim_amount, COUNT(DISTINCT cl.customer_id) as unique_customers_with_claims\n",
    "# TODO: FROM claims cl\n",
    "# TODO: GROUP BY YEAR(cl.claim_date), MONTH(cl.claim_date)\n",
    "# TODO: ORDER BY claim_year, claim_month\n",
    "# \n",
    "# Step 2: Execute the SQL query\n",
    "# TODO: Use spark.sql(monthly_trends_sql) to create the view\n",
    "# TODO: Print success message \"✅ Monthly trends view created\"\n",
    "# \n",
    "# EXPECTED OUTPUT: Monthly trends view created for Power BI time series analysis\n",
    "\n",
    "# Monthly Trends View (for Power BI time series)\n",
    "print(\"\uD83D\uDCCB Creating monthly trends view...\")\n",
    "monthly_trends_sql = f\"\"\"\n",
    "CREATE OR REPLACE VIEW {DATABASE_NAME}.monthly_trends AS\n",
    "SELECT \n",
    "    YEAR(cl.claim_date) as claim_year,\n",
    "    MONTH(cl.claim_date) as claim_month,\n",
    "    COUNT(*) as claims_count,\n",
    "    SUM(cl.claim_amount) as total_claim_amount,\n",
    "    AVG(cl.claim_amount) as avg_claim_amount,\n",
    "    COUNT(DISTINCT cl.customer_id) as unique_customers_with_claims\n",
    "FROM {DATABASE_NAME}.claims cl\n",
    "GROUP BY YEAR(cl.claim_date), MONTH(cl.claim_date)\n",
    "ORDER BY claim_year, claim_month\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(monthly_trends_sql)\n",
    "print(\"✅ Monthly trends view created\")\n",
    "\n",
    "print(\"\\n\uD83D\uDCCA POWER BI VIEWS SUMMARY:\")\n",
    "print(\"   ✅ customer_summary - Customer 360 view\")\n",
    "print(\"   ✅ policy_performance - Policy type analytics\")\n",
    "print(\"   ✅ monthly_trends - Time series analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "842aee4a-f44d-440a-ab00-630ea528ec92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Data Foundation Summary and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b5268d-81f1-4043-995b-f4fc961040d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83C\uDFAF DATA FOUNDATION SUMMARY\n==================================================\n\uD83D\uDCCA FOUNDATION METRICS:\n   \uD83D\uDCCB Customers: 15,000\n   \uD83D\uDCCB Policies: 75,000\n   \uD83D\uDCCB Claims: 10,643\n   \uD83D\uDCCB Payments: 178,013\n   \uD83D\uDCCB Interactions: 30,000\n\n\uD83D\uDD0D QUALITY ASSESSMENT:\n   Data Quality Score: 99.0/100\n   Business Rules: 4/5 passed\n   Relationship Integrity: ✅ PASS\n\n\uD83C\uDFD7️  INFRASTRUCTURE READINESS:\n   Database: insurance_analytics\n   Persistent Tables: 5/6 created\n   Power BI Views: 3/3 created\n\n\uD83D\uDE80 PIPELINE READINESS:\n   ✅ READY FOR DOWNSTREAM NOTEBOOKS\n   ✅ READY FOR ADF PIPELINE EXECUTION\n   ✅ READY FOR POWER BI INTEGRATION\n\n\uD83D\uDCCB NEXT STEPS:\n   1. ✅ Notebook 1: Customer Risk Profiling\n   2. ✅ Notebook 2: CLPV and Retention Modeling\n   3. ✅ Notebook 3: Executive Dashboard\n   4. ✅ ADF Pipeline: Sequential execution\n   5. ✅ Power BI: Dashboard creation\n\n✅ NOTEBOOK 0 COMPLETE - DATA FOUNDATION ESTABLISHED\n"
     ]
    }
   ],
   "source": [
    "# BUSINESS CONTEXT: Foundation summary validates entire data pipeline readiness\n",
    "# Comprehensive assessment ensures reliable analytics and business intelligence\n",
    "\n",
    "print(\"\uD83C\uDFAF DATA FOUNDATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Create comprehensive foundation metrics summary\n",
    "# \n",
    "# Step 1: Display foundation metrics\n",
    "# TODO: Print foundation metrics with record counts for all 5 datasets\n",
    "# TODO: Use f-string formatting with thousands separators (:,)\n",
    "# \n",
    "# Step 2: Display quality assessment\n",
    "# TODO: Print data quality score, business rules passed, and relationship integrity status\n",
    "# TODO: Use ternary operators for pass/fail status display\n",
    "# \n",
    "# Step 3: Display infrastructure readiness\n",
    "# TODO: Print database name, persistent tables count, and Power BI views count\n",
    "# \n",
    "# EXPECTED OUTPUT: Comprehensive foundation metrics and status summary\n",
    "\n",
    "# Foundation metrics\n",
    "print(\"\uD83D\uDCCA FOUNDATION METRICS:\")\n",
    "print(f\"   \uD83D\uDCCB Customers: {customers_df.count():,}\")\n",
    "print(f\"   \uD83D\uDCCB Policies: {policies_df.count():,}\")\n",
    "print(f\"   \uD83D\uDCCB Claims: {claims_df.count():,}\")\n",
    "print(f\"   \uD83D\uDCCB Payments: {payments_df.count():,}\")\n",
    "print(f\"   \uD83D\uDCCB Interactions: {interactions_df.count():,}\")\n",
    "\n",
    "# Quality assessment\n",
    "print(f\"\\n\uD83D\uDD0D QUALITY ASSESSMENT:\")\n",
    "print(f\"   Data Quality Score: {avg_quality:.1f}/100\")\n",
    "print(f\"   Business Rules: {passed_rules}/{total_rules} passed\")\n",
    "print(f\"   Relationship Integrity: {'✅ PASS' if customer_policy_ok and policy_claims_ok else '❌ FAIL'}\")\n",
    "\n",
    "# Infrastructure readiness\n",
    "print(f\"\\n\uD83C\uDFD7️  INFRASTRUCTURE READINESS:\")\n",
    "print(f\"   Database: {DATABASE_NAME}\")\n",
    "print(f\"   Persistent Tables: {successful_tables}/6 created\")\n",
    "print(f\"   Power BI Views: 3/3 created\")\n",
    "\n",
    "# BUSINESS CONTEXT: Pipeline readiness determines if downstream notebooks can execute\n",
    "# All critical components must be ready for reliable analytics pipeline\n",
    "\n",
    "# TODO: Assess overall pipeline readiness\n",
    "# \n",
    "# Step 1: Calculate pipeline readiness\n",
    "# TODO: Create pipeline_ready boolean combining data_quality_ok, business_rules_ok, table_creation_ok, and successful_tables >= 5\n",
    "# \n",
    "# Step 2: Display pipeline readiness status\n",
    "# TODO: If pipeline_ready, print success messages for downstream notebooks, ADF pipeline, and Power BI\n",
    "# TODO: Else print critical issues detected and pipeline execution may fail warnings\n",
    "# \n",
    "# Step 3: Display next steps\n",
    "# TODO: Print next steps list: Notebook 1 (Risk Profiling), Notebook 2 (CLPV), Notebook 3 (Executive Dashboard), ADF Pipeline, Power BI\n",
    "# \n",
    "# EXPECTED OUTPUT: Pipeline readiness assessment with next steps guidance\n",
    "\n",
    "# Pipeline readiness check\n",
    "pipeline_ready = (\n",
    "    data_quality_ok and \n",
    "    business_rules_ok and \n",
    "    table_creation_ok and \n",
    "    successful_tables >= 5\n",
    ")\n",
    "\n",
    "print(f\"\\n\uD83D\uDE80 PIPELINE READINESS:\")\n",
    "if pipeline_ready:\n",
    "    print(\"   ✅ READY FOR DOWNSTREAM NOTEBOOKS\")\n",
    "    print(\"   ✅ READY FOR ADF PIPELINE EXECUTION\")\n",
    "    print(\"   ✅ READY FOR POWER BI INTEGRATION\")\n",
    "else:\n",
    "    print(\"   ❌ CRITICAL ISSUES DETECTED\")\n",
    "    print(\"   ⚠️  PIPELINE EXECUTION MAY FAIL\")\n",
    "\n",
    "# Next steps\n",
    "print(f\"\\n\uD83D\uDCCB NEXT STEPS:\")\n",
    "print(\"   1. ✅ Notebook 1: Customer Risk Profiling\")\n",
    "print(\"   2. ✅ Notebook 2: CLPV and Retention Modeling\")\n",
    "print(\"   3. ✅ Notebook 3: Executive Dashboard\")\n",
    "print(\"   4. ✅ ADF Pipeline: Sequential execution\")\n",
    "print(\"   5. ✅ Power BI: Dashboard creation\")\n",
    "\n",
    "print(f\"\\n✅ NOTEBOOK 0 COMPLETE - DATA FOUNDATION ESTABLISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd26a6d-1954-4d2c-a488-feb26d7d256f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. Table Catalog for Downstream Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90259da1-5d5d-4938-91d2-c1dcfc5f66ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCDA TABLE CATALOG FOR DOWNSTREAM NOTEBOOKS\n==================================================\n\uD83D\uDDC4️  PERSISTENT TABLES AVAILABLE:\n   • insurance_analytics.customers\n   • insurance_analytics.policies\n   • insurance_analytics.claims\n   • insurance_analytics.payments\n   • insurance_analytics.interactions\n   • insurance_analytics.market_rates\n\n\uD83D\uDCCA POWER BI VIEWS AVAILABLE:\n   • insurance_analytics.customer_summary\n   • insurance_analytics.policy_performance\n   • insurance_analytics.monthly_trends\n\n\uD83D\uDCBB SAMPLE DATA ACCESS:\n   # Load customers in downstream notebooks:\n   customers_df = spark.table('insurance_analytics.customers')\n   \n   # Load policies in downstream notebooks:\n   policies_df = spark.table('insurance_analytics.policies')\n\n\uD83D\uDD0D VALIDATION CHECK:\n   ✅ Tables accessible: 15,000 customers, 75,000 policies\n\n\uD83C\uDFAF FOUNDATION COMPLETE - READY FOR ANALYTICS PIPELINE\n"
     ]
    }
   ],
   "source": [
    "# BUSINESS CONTEXT: Clear table catalog ensures downstream notebooks can access all required data\n",
    "# Proper documentation prevents integration issues and supports team collaboration\n",
    "\n",
    "print(\"\uD83D\uDCDA TABLE CATALOG FOR DOWNSTREAM NOTEBOOKS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Document available tables and access patterns\n",
    "# \n",
    "# Step 1: Display persistent tables available\n",
    "# TODO: Print list of all 6 persistent tables in DATABASE_NAME\n",
    "# TODO: Include customers, policies, claims, payments, interactions, market_rates\n",
    "# \n",
    "# Step 2: Display Power BI views available\n",
    "# TODO: Print list of all 3 Power BI views in DATABASE_NAME\n",
    "# TODO: Include customer_summary, policy_performance, monthly_trends\n",
    "# \n",
    "# Step 3: Provide sample data access code\n",
    "# TODO: Print example code for loading customers and policies in downstream notebooks\n",
    "# TODO: Use spark.table() function with full table names\n",
    "# \n",
    "# Step 4: Validate table accessibility\n",
    "# TODO: Try to access customers and policies tables and get counts\n",
    "# TODO: Print validation check results with customer and policy counts\n",
    "# TODO: Use try/except to handle any access errors\n",
    "# \n",
    "# EXPECTED OUTPUT: Complete table catalog with access examples and validation\n",
    "\n",
    "# Display available tables for next notebooks\n",
    "print(\"\uD83D\uDDC4️  PERSISTENT TABLES AVAILABLE:\")\n",
    "print(f\"   • {DATABASE_NAME}.customers\")\n",
    "print(f\"   • {DATABASE_NAME}.policies\") \n",
    "print(f\"   • {DATABASE_NAME}.claims\")\n",
    "print(f\"   • {DATABASE_NAME}.payments\")\n",
    "print(f\"   • {DATABASE_NAME}.interactions\")\n",
    "print(f\"   • {DATABASE_NAME}.market_rates\")\n",
    "\n",
    "print(f\"\\n\uD83D\uDCCA POWER BI VIEWS AVAILABLE:\")\n",
    "print(f\"   • {DATABASE_NAME}.customer_summary\")\n",
    "print(f\"   • {DATABASE_NAME}.policy_performance\")\n",
    "print(f\"   • {DATABASE_NAME}.monthly_trends\")\n",
    "\n",
    "# Sample data access for downstream notebooks\n",
    "print(f\"\\n\uD83D\uDCBB SAMPLE DATA ACCESS:\")\n",
    "print(f\"   # Load customers in downstream notebooks:\")\n",
    "print(f\"   customers_df = spark.table('{DATABASE_NAME}.customers')\")\n",
    "print(f\"   \")\n",
    "print(f\"   # Load policies in downstream notebooks:\")\n",
    "print(f\"   policies_df = spark.table('{DATABASE_NAME}.policies')\")\n",
    "\n",
    "# Quick validation that tables are accessible\n",
    "print(f\"\\n\uD83D\uDD0D VALIDATION CHECK:\")\n",
    "try:\n",
    "    customer_count = spark.table(f\"{DATABASE_NAME}.customers\").count()\n",
    "    policy_count = spark.table(f\"{DATABASE_NAME}.policies\").count()\n",
    "    print(f\"   ✅ Tables accessible: {customer_count:,} customers, {policy_count:,} policies\")\n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Table access error: {e}\")\n",
    "\n",
    "print(f\"\\n\uD83C\uDFAF FOUNDATION COMPLETE - READY FOR ANALYTICS PIPELINE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b63e179-b745-49ef-bbed-26be7594309e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "### ✅ Data Foundation Established:\n",
    "1. **Comprehensive Data Loading** - All 6 insurance datasets loaded and validated\n",
    "2. **Quality Assurance** - Data quality scoring and business rules validation\n",
    "3. **Persistent Storage** - Database tables created for ADF pipeline reliability\n",
    "4. **Power BI Integration** - Optimized views created for dashboard connectivity\n",
    "5. **Pipeline Readiness** - Infrastructure prepared for sequential notebook execution\n",
    "\n",
    "### \uD83C\uDFD7️ Infrastructure Created:\n",
    "- **Database**: `insurance_analytics` with 6 persistent tables\n",
    "- **Tables**: Customer, Policy, Claims, Payments, Interactions, Market Rates\n",
    "- **Views**: Customer Summary, Policy Performance, Monthly Trends\n",
    "- **Optimization**: Table optimization and indexing where applicable\n",
    "\n",
    "### \uD83D\uDE80 Ready for Pipeline:\n",
    "- **Notebook 1**: Customer Risk Profiling (uses persistent tables)\n",
    "- **Notebook 2**: CLPV and Retention Modeling (builds on Notebook 1 outputs)\n",
    "- **Notebook 3**: Executive Dashboard (integrates all previous analytics)\n",
    "- **ADF Pipeline**: Sequential execution with reliable data persistence\n",
    "- **Power BI**: Dashboard creation using optimized views\n",
    "\n",
    "### \uD83D\uDCCA Quality Metrics:\n",
    "- **Data Quality Score**: High completeness and consistency\n",
    "- **Business Rules**: Comprehensive validation passed\n",
    "- **Relationship Integrity**: Foreign key relationships validated\n",
    "- **Infrastructure**: All persistent tables and views created successfully\n",
    "\n",
    "### \uD83C\uDFAF Learning Outcomes Achieved:\n",
    "- **Data Engineering Foundation**: Comprehensive data loading and validation\n",
    "- **Quality Assurance**: Business rules implementation and testing\n",
    "- **Database Management**: Persistent table creation and optimization\n",
    "- **Pipeline Architecture**: Infrastructure preparation for enterprise workflows\n",
    "- **Business Intelligence**: Data preparation for executive dashboards"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nbk_0_data_validation_student",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}